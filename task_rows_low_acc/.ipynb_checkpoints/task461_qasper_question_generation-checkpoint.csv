id,task_name,task_family,definition,inputs,targets,output_text,latency_sec
task461-5c03d327094a465180c80e0db40d7164,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions."," Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability.",Where does the data in CoLA come from?,What datasets are used for the acceptability task?,0.10994338989257812
task461-4b10cd1d4b0a465f92b41e0cdab5006d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Proposed Fusion Techniques ::: Step-Wise Decoder Fusion
Our first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process. Proposed Fusion Techniques ::: Multimodal Attention Modulation
Similar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well. Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer
In terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction.",What are 3 novel fusion techniques that are proposed?,What are the proposed fusion techniques?,0.10005128383636475
task461-1ad40e2b9dc04634814e3d6bb85a2f3c,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Our study focused primarily on English tweets, since this was the language of our diagnostic training sample.",Do the authors report results only on English datasets?,What language is used for the dataset?,0.10147428512573242
task461-582228c9be534927ab4ca2688c258ada,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The corpus consists of a collection of 2,346 clinical notes (admission notes, progress notes, and discharge summaries), which amounts to 2,372,323 tokens in total (an average of 1,011 tokens per note). All the notes were written in English and extracted from the EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA, all of whom had in their history at least one instance of 30-day readmission.",What is the dataset used?,How large is the corpus?,0.08846032619476318
task461-a9bdd7ebbc104c1a8957b58edf07951c,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","ch As continuous word embedding spaces exhibit similar structures across (even distant) languages BIBREF35, we use a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space.",What multilingual word representations are used?,What multilingual word representations are used?,0.08253586292266846
task461-daa51875eae549bdabe3819cfb178ee6,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. ",What data was presented to the subjects to elicit event-related responses?,What datasets are used?,0.06652361154556274
task461-3d2960ee6c3048c5bade3c2e8ecb1776,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions."," In Section SECREF16, we first provide more details about the experimental setting that we followed.  As explained in Section SECREF3, we used BabelNet BIBREF29 as our reference taxonomy. BabelNet is a large-scale full-fledged taxonomy consisting of heterogeneous sources such as WordNet BIBREF36, Wikidata BIBREF37 and WiBi BIBREF38, making it suitable to test our hypothesis in a general setting.  To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing. To tune the prior probability $\lambda _A$ for these categories, we hold out 10% from the training set as a validation set.",What experiments they perform to demonstrate that their approach leads more accurate region based representations?,What datasets are used?,0.06655853986740112
task461-17044f16d2dc4974b537af0c53a1bf3b,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In this paper, we propose three different types of CRU models: shallow fusion, deep fusion and deep-enhanced fusion, from the most fundamental one to the most expressive one. The most simple one is to directly apply a CNN layer after the embedding layer to obtain blended contextual representations. Then a GRU layer is applied afterward.",How is CNN injected into recurent units?,What are the different types of CRU models?,0.1017645001411438
task461-0729833529a84e978fdf766231909f65,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In Spanish, serigos2017using extracted anglicisms from a corpus of Argentinian newspapers by combining dictionary lookup (aided by TreeTagger and the NLTK lemmatizer) with automatic filtering of capitalized words and manual inspection. In serigos2017applying, a character n-gram module was added to estimate the probabilities of a word being English or Spanish. moreno2018configuracion used different pattern-matching filters and lexicon lookup to extract anglicism cadidates from a corpus of tweets in US Spanish.",Does the paper mention other works proposing methods to detect anglicisms in Spanish?,How are the anglicisms extracted?,0.12877219915390015
task461-4554b3d8382e4c52b20037202d919b43,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Below, we explore different ways to back off when the ScRNN predicts UNK (a frequent outcome for rare and unseen words):

Pass-through: word-recognizer passes on the (possibly misspelled) word as is.

Backoff to neutral word: Alternatively, noting that passing $\colorbox {gray!20}{\texttt {UNK}}$ -predicted words through unchanged exposes the downstream model to potentially corrupted text, we consider backing off to a neutral word like `a', which has a similar distribution across classes.

Backoff to background model: We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.",How do the backoff strategies work?,How do they back off when the ScRNN predicts UNK?,0.17498797178268433
task461-a909a4bd2ce54e4684a5fe6a6a142ff4,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Even among researchers familiar with the definitions outlined above, there was still a low level of agreement (Krippendorff's INLINEFORM0 ).",How was reliability measured?,What is the consensus on the definitions?,0.15898901224136353
task461-685ab0e5da3a4df3a85683e944187f3f,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","For instance, question and aux are correlated because main-clause questions in English require subject-aux inversion and in many cases the insertion of auxiliary do (Do lions meow?).  Expletives, or “dummy” arguments, are semantically inert arguments. The most common expletives in English are it and there, although not all occurrences of these items are expletives.",Do they report results only on English data?,What are the most common expletives in English?,0.3681532144546509
task461-15c6991e0f5b4baea3a5d4fa95c19b97,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We thus also conducted human studies on Amazon MTurk to evaluate the generated responses with pairwise comparison for dialogue quality. We compare our models with an advanced decoding algorithm MMI BIBREF2 and two models, namely LSTM BIBREF0 and VHRED BIBREF7, both with additive attention. To our best knowledge, LSTM and VHRED were the primary models with which F1's were reported on the Ubuntu dataset. Following BIBREF5 (BIBREF5), we employ two criteria: Plausibility and Content Richness. The first criterion measures whether the response is plausible given the context, while the second gauges whether the response is diverse and informative. ","How is human evaluation performed, what was the criteria?",What are the baselines used?,0.15199238061904907
task461-8f0581b96afc4f399f606effb18929b1,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In phase 1 (also referred to as the “exploration” phase) the algorithm explores the state space through keeping track of previously visited states by maintaining an archive. During this phase, instead of resuming the exploration from scratch, the algorithm starts exploring from promising states in the archive to find high performing trajectories.",How is trajectory with how rewards extracted?,What is the initial state exploration phase?,0.2637318968772888
task461-d456f0bc378f47acbe52fcc9ac8bbc63,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Using the fine-grained gating mechanism conditioned on the lexical features, we can accurately control the information flows between word-level and char-level. Intuitively, the formulation is as follows: INLINEFORM0

where INLINEFORM0 is the element-wise multiplication operator. when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place. It is practical in real scenarios. For example, for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure.",How does the gatint mechanism combine word and character information?,How is the gating mechanism based?,0.08818840980529785
task461-3506af1022794681913e8c12890b294f,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",Each game's video ranges from 30 to 50 minutes in length which contains image and chat data linked to the specific timestamp of the game.,What is the average length of the recordings?,How long is the video?,0.11573660373687744
task461-cc573bff34884b848de5719c751febc3,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The learning model for retrieval is trained by an oracle constructed using distant supervision. Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question. First, the question and article are embedded into vector sequences, using the same method as the comprehension model. We do not use anonymization here, to retain simplicity. Otherwise, the anonymization procedure would have to be repeated several times for a potentially large collection of documents. These vector sequences are next fed to a Bi-GRU, to produce the outputs $v$ (for the question) and $H_c$ (for the document) similar to the previous section.",How can a neural model be used for a retrieval if the input is the entire Wikipedia?,How is the model trained?,0.060032010078430176
task461-8caf72d48e2d420283fdcbf3039630f3,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text.,How do they extract topics?,How were the sentiment scores obtained?,0.0652574896812439
task461-d6081ca4fce049a8adfb7cfed6c10fc6,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.",what are the topics pulled from Reddit?,What datasets are used?,0.15416371822357178
task461-772a451eefef4443b5aef8cecf6c7848,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the Entity Network BIBREF17 and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with $T$ sentences, where each sentence consists of a sequence of words represented with $K$ -dimensional word embeddings $\lbrace e_1, \ldots , e_N\rbrace $ , a question on the document represented as another sequence of words and an answer to the question.",How is knowledge stored in the memory?,What is the architecture of the model?,0.11670452356338501
task461-9bd3778aae0a497f8007aa70425cd6ce,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","To create our training dataset, we followed an approach similar to LASER. The dataset contains 6 languages: English, Spanish, German, Dutch, Korean and Chinese Mandarin. The dataset was created by using translations provided by Tatoeba and OpenSubtitles BIBREF16.",Which corpus do they use?,What languages are used in the training dataset?,0.11673766374588013
task461-fe8db0859fba4913b992f625fc98e0d2,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Following lebret2016neural, we used BLEU-4, NIST-4 and ROUGE-4 as the evaluation metrics.",What metrics are used for evaluation?,What evaluation metrics are used?,0.0902707576751709
task461-0e9b5a3b420b467b8ea528c9c28e45b1,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin.  Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks.",What are the specific tasks being unified?,What datasets are used?,0.35311681032180786
task461-7968ed18d66740c698ad310eba7282ac,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","S2-GORC is a large citation graph dataset which includes full texts of 8.1 million scientific documents. We select a subset of 154K computer science articles as our corpus. From these, we extract 622K citing sentences that link back to other documents in our corpus. We hold 2500 examples for each of the validation and test sets.",What is the size of the corpus?,What dataset do they use?,0.1306898593902588
task461-419e40315e0049a4b170a7a19b8bc96e,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",We evaluate the quality of the document embeddings learned by the different variants of CAHAN and the HAN baseline on three of the large-scale document classification datasets introduced by BIBREF14 and used in the original HAN paper BIBREF5.,Do they compare to other models appart from HAN?,What datasets are used?,0.06668537855148315
task461-3ca7f9e06bd54473a6575bf09e9e6151,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","BERT: We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert). We also experiment with a two-stage classifier, where we separately train the model on questions only to predict answerability. At inference time, if the answerable classifier predicts the question is answerable, the evidence identification classifier produces a set of candidate sentences (Bert + Unanswerable).

",What type of neural model was used?,What baselines are used?,0.058103322982788086
task461-140813266efc41258a796c2e5c1e2d8b,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We have created a dataset of discharge summaries and nursing notes, all in the English language, with a focus on frequently readmitted patients, labeled with 15 clinical patient phenotypes believed to be associated with risk of recurrent Intensive Care Unit (ICU) readmission per our domain experts (co-authors LAC, PAT, DAG) as well as the literature. BIBREF10 BIBREF11 BIBREF12",How many different phenotypes are present in the dataset?,What datasets are used?,0.14434027671813965
task461-8f585b78f8654be995c690c8a8d01282,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The corpus used for sentiment analysis is the IMDb dataset of movie reviews by BIBREF11 while that for NER is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples. ",What Named Entity Recognition dataset is used?,What datasets are used for sentiment analysis?,0.07734960317611694
task461-a0a6fb1a32584a8bb5c5ae48be795355,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions."," Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss. ",What neural language models are explored?,What baseline is used?,0.05287200212478638
task461-095cfef92a1f469a863a168a43ff83aa,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In an offline step, we organize the content of each instance in a graph where each node represents a sentence from the passages and the question. Then, we add edges between nodes using the following topology:

we fully connect nodes that represent sentences from the same passage (dotted-black);

we fully connect nodes that represent the first sentence of each passage (dotted-red);

we add an edge between the question and every node for each passage (dotted-blue).",How are some nodes initially connected based on text structure?,How are the questions connected to the graph?,0.09401148557662964
task461-d20e3e89c0484ae89ad3f312dce8ebf4,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Internally, the ASR system maintains a rich hypothesis space in the form of speech lattices or confusion networks (cnets).",What is a word confusion network?,What is the internal architecture of the ASR system?,0.08837294578552246
task461-076139f6f640456bb12ce84fb15da926,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese. Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. Contributors record voice clips by reading from a bank of donated sentences",How was the dataset collected?,What languages are used in the corpus?,0.09337550401687622
task461-9c636e3987784892a6c737e634a970f7,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","As shown in Table TABREF3 , we collect Amazon review keywords for 2,896 e-books (publishers: Kiwi, Rowohlt, Fischer, and Droemer), which leads to 33,663 distinct review keywords and on average 30 keyword assignments per e-book.",how large is the vocabulary?,How many e-books are used?,0.07868409156799316
task461-c9a09c781da24cdd902c93caeb8fa97b,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21 , combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM.",Which shallow approaches did they experiment with?,What baselines are used?,0.0790819525718689
task461-bc2b4c786b2d47639868e95350f5906a,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Our approach improves LR by 5.17% (Accuracy) and 18.38% (AUC), and MLP by 10.71% (Accuracy) and 30.27% (AUC) on average. Such significant improvements clearly demonstrate that our approach is effective at improving model performance.",How are the accuracy merits of the approach demonstrated?,How does the approach improve model performance?,0.08600473403930664
task461-883b9319ba014065aa94fe2fdb495236,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We use a siamese neural network, shown to perform state-of-the-art few-shot learning BIBREF11, as our baseline model. We modify the original to account for sequential data, with each twin composed of an embedding layer, a Long-Short Term Memory (LSTM) BIBREF12 layer, and a feed-forward layer with Rectified Linear Unit (ReLU) activations.",What were the baselines?,What is the baseline model?,0.06784200668334961
task461-124b9355aebd4c9c8435d07f5cb33a13,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Therefore, we just implement a simple one-layer RNN with LSTM cell to classify pre-processed sentences into ironic sentences and non-ironic sentences because LSTM networks are widely used in irony detection.",How did the authors find ironic data on twitter?,What is the baseline used?,0.08962267637252808
task461-edfd60a38ce54276a4ffb019845fd5a8,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Baseline Results ::: Speech Synthesis
In our previous work on building speech systems on found data in 700 languages, BIBREF7, we addressed alignment issues (when audio is not segmented into turn/sentence sized chunks) and correctness issues (when the audio does not match the transcription). We used the same techniques here, as described above.

For the best quality speech synthesis we need a few hours of phonetically-balanced, single-speaker, read speech. Our first step was to use the start and end points for each turn in the dialogues, and select those of the most frequent speaker, nmlch. This gave us around 18250 segments. We further automatically removed excessive silence from the start, middle and end of these turns (based on occurrence of F0). This gave us 13 hours and 48 minutes of speech.

We phonetically aligned this data and built a speech clustergen statistical speech synthesizer BIBREF9 from all of this data. For speech recognition (ASR) we used Kaldi BIBREF11. We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state-of-the-art Transformer architecture BIBREF14 with the toolkit of BIBREF15.",What are the models used for the baseline of the three NLP tasks?,What baselines are used?,0.07095950841903687
task461-3d6b9ef399384ea5b522e7acfc222001,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score. The automatic score roughly followed the design in BIBREF13. The pitch tracker of librosa package BIBREF18 was employed to extract pitch information of the input and output audio. Then the output pitch was compared to the input pitch using the normalized cross correlation (NCC) which would give a score between 0 and 1. The higher the score is, the better the output pitch matches the input pitch. We conducted the evaluation on USVC (our) and PitchNet. The evaluated automatic scores on conversion and reconstruction tasks are shown in Tab. TABREF14. Our method performed better both on conversion and reconstruction. The scores of reconstruction are higher than conversion since both models were trained using a reconstruction loss. However, the score of our method on conversion is even higher than the score of USVC (Our) on reconstruction. Mean Opinion Score (MOS) was used as a subjective metric to evaluate the quality of the converted audio. Two questions were asked: (1) what is quality of the audio? (naturalness) (2) How well does the converted version match the original? (similarity) A score of 1-5 would be given to answer the questions. The evaluation was conducted on USVC (Our) and PitchNet. Besides, the converted samples provided by BIBREF0 was also included to give a more convincing evaluation. As shown by Tab. TABREF15, the naturalness and similarity of our method are both higher than the other two ones. Our implementation of USVC performed slightly lower than the original author's because we cannot fully reproduce the results of them. Next we qualitatively analyze the influence of input pitch in our method. We used different pitch as input to observe how the output pitch would change along with the input pitch. The input pitch was multiplied by 0.7, 1.0 and 1.3 respectively. And the output pitch was also extracted by the pitch tracker of the librosa package. Fig. FIGREF16 plots the pitch of input audio and output audio with different pitch as input while keeping the target singer the same. As shown by Fig. FIGREF16, the output pitch changes significantly along with the input pitch. The examples are also presented at our website.",How is the quality of singing voice measured?,How are the outputs evaluated?,0.10013234615325928
task461-4e320a1c33ab4fc6a74f13361d5d9b50,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).",What metrics are used in evaluation of this task?,What evaluation metrics are used?,0.3903747797012329
task461-894d309f0b144db582a59c1bf11ad910,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which I have generated 622,144 sentences after filtering out sentences with lower quality.",What dataset is used?,How many sentences are generated?,0.06756085157394409
task461-980c64a3af5744ae9969025298bf25dc,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest...

On the other hand, just like in particle physics where we have particles and anti-particles, the atomic types include types as well as anti-types. But unlike in particle physics, there are two kinds of anti-types, namely left ones and right ones. This makes language even more non-commutative than quantum theory!",Do they break down word meanings into elementary particles as in the standard model of quantum theory?,What are the atomic grammatical types?,0.12068235874176025
task461-4546f0b4bcf14b70819e946156a1dd6c,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",Single-Turn: This dataset consists of single-turn instances of statements and responses from the MiM chatbot developed at Constellation AI BIBREF5. Multi-Turn: This dataset is taken from the ConvAI2 challenge and consists of various types of dialogue that have been generated by human-computer conversations.,what datasets did they use?,What datasets are used?,0.09704643487930298
task461-8a9cd324fb4341189728905fb99fd2d7,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",We have also included the unlabelled set of 2472 telephone calls from both minor (Cebuano and Mandarin) and major (Tagalog and Cantonese) languages provided by NIST in the system training.,Which are the novel languages on which SRE placed emphasis on?,What languages are used in the unlabelled training?,0.11668872833251953
task461-de6b7fdcf46e420e8b68359b402ca6dc,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector.",How is knowledge retrieved in the memory?,What is the model architecture?,0.19157791137695312
task461-74377a0030604ce8a120ed9ad1ee481c,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables. ",Which additional latent variables are used in the model?,How are crosslingual latent variables added to the monolingual model?,0.12187838554382324
task461-76ad051d980f414092523dc9d4b84e04,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","All documents are segmented into paragraphs and processed at the paragraph level (both training and inference); this is acceptable because we observe that most paragraphs are less than 200 characters. The input sequences are segmented by the BERT tokenizer, with the special [CLS] token inserted at the beginning and the special [SEP] token added at the end.",Was the structure of regulatory filings exploited when training the model? ,How are the documents segmented?,0.0766458511352539
task461-f54c00a54229461e9145d517cfaeed4b,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",Our neural network model mainly consists of three parallel LSTM BIBREF21 layers.,What architecture has the neural network?,What is the architecture of the model?,0.3421561121940613
task461-f989d62ea9cb4bf1b2a29015497207f8,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. ,Where is a question generation model used?,What is the model used for question generation?,0.34489011764526367
task461-949ecbd818054f6b951ab065c30abd36,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Construction of the auxiliary sentence
For simplicity, we mainly describe our method with TABSA as an example.

We consider the following four methods to convert the TABSA task into a sentence pair classification task:

The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same. For example, for the set of a target-aspect pair (LOCATION1, safety), the sentence we generate is “what do you think of the safety of location - 1 ?""

For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler. The sentence created at this time is not a standard sentence, but a simple pseudo-sentence, with (LOCATION1, safety) pair as an example: the auxiliary sentence is: “location - 1 - safety"".

For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution. At this time, each target-aspect pair will generate three sequences such as “the polarity of the aspect safety of location - 1 is positive"", “the polarity of the aspect safety of location - 1 is negative"", “the polarity of the aspect safety of location - 1 is none"". We use the probability value of INLINEFORM1 as the matching score. For a target-aspect pair which generates three sequences ( INLINEFORM2 ), we take the class of the sequence with the highest matching score for the predicted category.

The difference between NLI-B and QA-B is that the auxiliary sentence changes from a question to a pseudo-sentence. The auxiliary sentences are: “location - 1 - safety - positive"", “location - 1 - safety - negative"", and “location - 1 - safety - none"".

After we construct the auxiliary sentence, we can transform the TABSA task from a single sentence classification task to a sentence pair classification task. As shown in Table TABREF19 , this is a necessary operation that can significantly improve the experimental results of the TABSA task.",How do they generate the auxiliary sentence?,How are the auxiliary sentences generated?,0.10092031955718994
task461-99afbacd76094760b3495943251cda37,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",We use English translations of the Chinese source texts in the WMT 2017 English–Chinese test set BIBREF18 for all experiments presented in this article:,What languages do they investigate for machine translation?,What datasets are used?,0.07122862339019775
task461-cc4d48451731477ebd61bc96a63da7bb,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We adopt the mixture of two multilingual translation corpus as our training data: MultiUN BIBREF20 and OpenSubtitles BIBREF21. MultiUN consists of 463,406 official documents in six languages, containing around 300M words for each language. OpenSubtitles is a corpus consisting of movie and TV subtitles, which contains 2.6B sentences over 60 languages. We select four shared languages of the two corpora: English, Spanish, Russian and Chinese. Statistics of the training corpus are shown in Table TABREF14.",What multilingual parallel data is used for training proposed model?,What datasets are used?,0.11866813898086548
task461-49b7a8941a6545798a5abdad626c7bfd,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks.

While the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12.",What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?,What is the text model used?,0.30123329162597656
task461-bc37e8c38d014185aa526da5a4d6f273,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","IDEA
BIBREF9 Two different BERT models were developed. For Friends, pre-training was done using a sliding window of two utterances to provide dialogue context. Both Next Sentence Prediction (NSP) phase on the complete unlabeled scripts from all 10 seasons of Friends, which are available for download. In addition, the model learned the emotional disposition of each of six main six main characters in Friends (Rachel, Monica, Phoebe, Joey, Chandler and Ross) by adding a special token to represent the speaker. For EmotionPush, pre-training was performed on Twitter data, as it is similar in nature to chat based dialogues. In both cases, special attention was given to the class imbalance issue by applying “weighted balanced warming” on the loss function.",What model was used by the top team?,What datasets were used?,0.1043386459350586
task461-52816c39ee744ae89ae37585995431a5,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words.  We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words. ",Do they model semantics ,How do they model the data?,0.07446157932281494
task461-a12f8afd3f7b410abe87659569195dc9,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In addition, in order to estimate annotation reliability and provide for better evaluation, every question in the test set is answered by at least two additional experts.",Are the answers double (and not triple) annotated?,How many additional experts are used?,0.07146584987640381
task461-7507350ef8e74b998a5e28485681d61f,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Also, we compared our models with other existing works on this dataset including OpATT BIBREF6 and Neural Content Planning with conditional copy (NCP+CC) BIBREF4.",What is the state-of-the-art model for the task?,What other existing models are used?,0.1328984498977661
task461-08aa1ce006dd426da5b85e1c39e65bb1,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Different from scaled dot-product attention, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention. We assume that the Gaussian weight only relys on the distance between characters.",How does Gaussian-masked directional multi-head attention works?,What is the weight of the Gaussian weight?,0.161493718624115
task461-fb66bcbcf9d94fcfa587695641510644,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Since the training data consists only of utterance-denotation pairs, the ranker is trained to maximize the log-likelihood of the correct answer $z$ by treating logical forms as a latent variable The role of the ranker is to score the candidate logical forms generated by the parser; at test time, the logical form receiving the highest score will be used for execution. The ranker is a discriminative log-linear model over logical form $y$ given utterance $x$ : ",How does the model compute the likelihood of executing to the correction semantic denotation?,What is the ranker used for?,0.1984807848930359
task461-9d0c245de393412fae504b95e1e0c92d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In this subsection, we see the influence of each component of a model on performance by removing or replacing its components. the SNLI dataset is used for experiments, and the best performing configuration is used as a baseline for modifications. We consider the following variants: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0 , (iii) models without INLINEFORM1 , and (iv) models that integrate lower contexts via peephole connections.",What were the baselines?,What are the different ways of evaluating the model?,0.3906260132789612
task461-3254fa35325a4c7a9a5e602da6876f99,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In this work, we propose to augment the integer linear programming (ILP)-based summarization framework with a low-rank approximation of the co-occurrence matrix, and further evaluate the approach on a broad range of datasets exhibiting high lexical diversity.",What do they constrain using integer linear programming?,What is the proposed approach?,0.07257598638534546
task461-9a4da1b9d99c4685a7a3588e01c48874,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.",How do they extract causality from text?,What is the lexical model used?,0.4022286534309387
task461-ee4b48cb7aaa45f689f926fd445f3859,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12. The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names.",Do they report results only on English datasets?,What datasets are used?,0.16544795036315918
task461-6169356e010c481cb78d5af58da306e5,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes.,Does their model start with any prior knowledge of words?,What is the baseline?,0.3613893389701843
task461-e06265b4797e429faa1643bd9f86c52c,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section ""Dataset"" ).",Do they release MED?,What datasets are used?,0.08950883150100708
task461-b722230b316143c88abf55874fb1b24a,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","While BERT-QC achieves large gains over existing methods on the ARC dataset, here we demonstrate that BERT-QC also matches state-of-the-art performance on TREC BIBREF6 , while surpassing state-of-the-art performance on the GARD corpus of consumer health questions BIBREF3 and MLBioMedLAT corpus of biomedical questions BIBREF4 .",Which datasets are used for evaluation?,Which datasets do they compare their model to?,0.08280247449874878
task461-1ce18cbb9a654b379e95a141f16312a4,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","As the first step, we build three baseline LID systems, one based on the i-vector model, and the other two based on LSTM-RNN, using the speech data of two languages from Babel: Assamese and Georgian (AG). The two RNN LID baselines are: a standard RNN LID system (AG-RNN-LID) that discriminates between the two languages in its output, and a multi-task system (AG-RNN-MLT) that was trained to discriminate between the two languages as well as the phones.",Which is the baseline model?,What baselines are used?,0.09134906530380249
task461-e6b1f5d954b642789ff5421b608e9109,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","For de-identification tasks, the three metrics we will use to evaluate the performance of our architecture are Precision, Recall and INLINEFORM0 score as defined below.",What evaluation metrics do they use?,What metrics are used to evaluate the performance of the architecture?,0.09878069162368774
task461-bee1036b9f914e679615991c81fc7843,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). ",What topics are included in the debate data?,What topics are discussed in the CreateDebate dataset?,0.10046547651290894
task461-88bffe587e144c66933393058e926d35,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task,What embedding algorithm and dimension size are used?,What is the baseline used?,0.4671214818954468
task461-a854d8f113a74f3dbd96a4279bf192e6,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We carried out experiments on four Chinese NLP tasks, including Emotion Classification (EC), Named Entity Recognition (NER), Sentence Pair Matching (SPM) and Natural Language Inference (NLI).",What benchmarks did they experiment on?,What tasks are used?,0.053935885429382324
task461-5f8ea527d291489e9b9f685bad7d8a58,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question.","What is an ""answer style""?",What is the baseline model?,0.4250410795211792
task461-f0d263f8639a4639a58c5d688ef96d95,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework.,Do they compare against Noraset et al. 2017?,What baseline model is used?,0.14566344022750854
task461-e495c5fff0604fc898dc30ada3596139,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Clustering was performed separately for each specialty of doctors. We also examined the distribution of doctors' IDs in the obtained clusters. It turned out that some clusters covered almost exactly descriptions written by one doctor. This situation took place in the specialties where clusters are separated with large margins (e.g. psychiatry, pediatrics, cardiology).",Do they explore similarity of texts across different doctors?,What is the clustering method used?,0.0758664608001709
task461-daa51875eae549bdabe3819cfb178ee6,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. ",How many subjects does the EEG data come from?,What datasets are used?,0.11609631776809692
task461-a2fdd2db30b44405972afaa9539d0336,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions."," In this work, we develop a technique to rapidly transfer an existing pre-trained model from English to other languages in an energy efficient way BIBREF8. As the first step, we focus on building a bilingual language model (LM) of English and a target language. Starting from a pre-trained English LM, we learn the target language specific parameters (i.e., word embeddings), while keeping the encoder layers of the pre-trained English LM fixed. We then fine-tune both English and target model to obtain the bilingual LM. ",How is the model transferred to other languages?,What is the baseline model used?,0.0736163854598999
task461-1ca7224246214b16b5fe912c01150985,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Since our methodology does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines. We use this dataset to characterize Arabic offensive language to ascertain the topics, dialects, and users' gender that are most associated with the use of offensive language. ","In what way is the offensive dataset not biased by topic, dialect or target?",How large is the dataset?,0.29424983263015747
task461-bf81342858844ef991e4020b19133b82,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In this work, we build a novel regression model, based on linguistic, content, behavioral and topic features to detect Arabic Twitter bots to understand the impact of bots in spreading religious hatred in Arabic Twitter space. ",Do they propose a new model to better detect Arabic bots specifically?,What features do they use to detect Arabic Twitter bots?,0.09634649753570557
task461-532758b33f644b079e848792958df1df,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task.

",On which task does do model do worst?,What are the results of the joint task?,0.08324825763702393
task461-68a8b7e170bd4f0997d9a395a1b6244e,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Even if the optional lemmatization step is applied, one can still aim at further reducing the graph complexity by merging similar vertices. This step is called meta vertex construction. The meta-vertex construction step works as follows. Let INLINEFORM0 represent the set of vertices, as defined above. A meta vertex INLINEFORM1 is comprised of a set of vertices that are elements of INLINEFORM2 , i.e. INLINEFORM3 . Let INLINEFORM4 denote the INLINEFORM5 -th meta vertex. We construct a given INLINEFORM6 so that for each INLINEFORM7 , INLINEFORM8 's initial edges (prior to merging it into a meta vertex) are rewired to the newly added INLINEFORM9 . Note that such edges connect to vertices which are not a part of INLINEFORM10 . Thus, both the number of vertices, as well as edges get reduced substantially. This feature is implemented via the following procedure: Meta vertex candidate identification. Edit distance and word lengths distance are used to determine whether two words should be merged into a meta vertex (only if length distance threshold is met, the more expensive edit distance is computed). The meta vertex creation. As common identifiers, we use the stemmed version of the original vertices and if there is more than one resulting stem, we select the vertex from the identified candidates that has the highest centrality value in the graph and its stemmed version is introduced as a novel vertex (meta vertex).",How are meta vertices computed?,How is meta vertex construction done?,0.09037965536117554
task461-6fe26965406e4f04a6227c664c3e654e,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",We evaluated the proposed model using the publicly available LibriSpeech ASR corpus BIBREF23. The LibriSpeech dataset consists of 970 hours of audio data with corresponding text transcripts (around 10M word tokens) and an additional 800M word token text only dataset.,How big is LibriSpeech dataset?,What datasets are used?,0.8696494102478027
task461-c89ae1761f194c2d86f3f0ea5ce697fc,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. ,Do they evaluate only on English data?,How is the dataset used?,0.1740015149116516
task461-86c26a57f0934c21bc3ccd768fa1d9c9,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference.",How many layers are there in their model?,What is the model architecture?,0.07159155607223511
task461-c4781a6e12174587bbcb9101106a295d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",$LS \cup KLD \cup CONN$ and $KLD \cup LS \cup LS_{inter}$ are the best systems with the highest recall and F1-score respectively.,What was previous state-of-the-art approach?,What are the best systems?,0.08935403823852539
task461-06c34e1034424c0aa69982c9458390bf,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1 lists statistics of the two datasets.",What datasets are used to evaluate this paper?,What datasets are used?,0.06250447034835815
task461-8727de1ee76b4c0cb2265dd8f73d3d59,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","To compare our results, we use the provided baseline, which is a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs. ",What was the baseline model?,What baselines are used?,0.40609270334243774
task461-1ad9cba23e40408c92e30b6bcc9d9742,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\%$ against BERT's 72$\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. ",Should their approach be applied only when dealing with incomplete data?,What datasets are used?,0.8719874620437622
task461-945e13c8a0eb4720bf8655b59a5c41ac,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","On another note, we apply our formalization to evaluate multilingual $\textsc {bert} $'s syntax knowledge on a set of six typologically diverse languages. Although it does encode a large amount of information about syntax (more than $81\%$ in all languages), it only encodes at most $5\%$ more information than some trivial baseline knowledge (a type-level representation). This indicates that the task of POS labeling (word-level POS tagging) is not an ideal task for contemplating the syntactic understanding of contextual word embeddings. We see that—in all analysed languages—type level embeddings can already capture most of the uncertainty in POS tagging. We also see that BERT only shares a small amount of extra information with the task, having small (or even negative) gains in all languages. Finally, when put into perspective, multilingual $\textsc {bert} $'s representations do not seem to encode much more information about syntax than a trivial baseline. $\textsc {bert} $ only improves upon fastText in three of the six analysed languages—and even in those, it encodes at most (in English) $5\%$ additional information.",Was any variation in results observed based on language typology?,Does the proposed model outperform the baseline?,0.16457974910736084
task461-a8360d10a299451f88b4088bb593a5e5,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 .  The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks.",What methods is RelNet compared to?,How is the performance compared to the EntNet model?,0.10116434097290039
task461-e82fbaac9c514ceea14665dd741c05e3,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Validated transcripts were sent to professional translators. In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11. We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.",How is the quality of the data empirically evaluated? ,How are the translations evaluated?,0.2824735641479492
task461-d92528e221694868a9c93359e05a87c9,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We then train models on WMT English-German (EnDe) without BT noise, and instead explicitly tag the synthetic data with a reserved token. We repeat these experiments with WMT English-Romanian (EnRo), where NoisedBT underperforms standard BT and TaggedBT improves over both techniques. We perform our experiments on WMT18 EnDe bitext, WMT16 EnRo bitext, and WMT15 EnFr bitext respectively. We use WMT Newscrawl for monolingual data (2007-2017 for De, 2016 for Ro, 2007-2013 for En, and 2007-2014 for Fr). For bitext, we filter out empty sentences and sentences longer than 250 subwords. We remove pairs whose whitespace-tokenized length ratio is greater than 2. This results in about 5.0M pairs for EnDe, and 0.6M pairs for EnRo. We do not filter the EnFr bitext, resulting in 41M sentence pairs.",What datasets was the method evaluated on?,How many sentences are used?,0.18651270866394043
task461-eaf9a6a0cbc04b1686797f753693013d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The first one is a Turkish news-web corpus containing 423M words and 491M tokens, namely the BOUN Web Corpus BIBREF9 , BIBREF10 . The second one is composed of 21M Turkish tweets with 241M words and 293M tokens, where we combined 1M tweets from TS TweetS by Sezer-2013 and 20M Turkish Tweets by Bolat and Amasyalı.",What data was used to build the word embeddings?,What datasets are used?,0.05885756015777588
task461-50fffa5a22c9497b99d62c6d883f81d5,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities BIBREF1 , BIBREF2 , general text-based attributes BIBREF3 , descriptive text of images BIBREF4 , nodes in graph structure of networks BIBREF5 , and queries BIBREF6 , to name a few.",What domains are considered that have such large vocabularies?,What other tasks are they able to perform?,0.11241352558135986
task461-1b4cfbcd698b4fa6a44fec783e641476,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We present here a step in this direction: a probabilistic semantic parser that uses a large knowledge base (NELL) to form a prior probability distribution on the meanings of sentences it parses, and that ""understands"" each sentence either by identifying its existing beliefs that correspond to the sentence's meaning, or by creating new beliefs.",What knowledge bases do they use?,What is the baseline?,0.11241352558135986
task461-594f6f7cc9424a388c9ced62d9c5da8e,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.",Does the model have attention?,What is the LSTM decoder used for?,0.20793366432189941
task461-e7c00a9538cb48398e91f82cc0d7f56e,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Here we describe the components of our probabilistic model of question generation.  The details of optimization are as follows. First, a large set of 150,000 questions is sampled in order to approximate the gradient at each step via importance sampling. Second, to run the procedure for a given model and training set, we ran 100,000 iterations of gradient ascent at a learning rate of 0.1.",Is it a neural model? How is it trained?,How is the model trained?,0.1410667896270752
task461-87e0219f23304c6cbc7d6a7b49aa9ccc,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. ",Are synonymous relation taken into account in the Japanese-Vietnamese task?,What is the method used to replace OOV words?,0.09469789266586304
task461-6e30f2b7397f456abb150ea771539087,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions."," We also use the final hidden layer of the neural network as a task-specific embedding of the claim, together with the Web evidence. ",What data is used to build the task-specific embeddings?,What is the task-specific embedding of the claim?,0.09836775064468384
task461-8934938ecaf3492498982917315a5d54,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Each classifier is implemented with the following specifications:

Naïve Bayes (NB): Multinomial NB with additive smoothing constant 1

Logistic Regression (LR): Linear LR with L2 regularization constant 1 and limited-memory BFGS optimization

Support Vector Machine (SVM): Linear SVM with L2 regularization constant 1 and logistic loss function

Random Forests (RF): Averaging probabilistic predictions of 10 randomized decision trees

Gradient Boosted Trees (GBT): Tree boosting with learning rate 1 and logistic loss function Along with traditional machine learning approaches, we investigate neural network based models to evaluate their efficacy within a larger dataset. In particular, we explore Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variant models.",What learning models are used on the dataset?,What are the proposed classifiers?,0.07537329196929932
task461-f26f2500ca474d5884bc6b048ca59652,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","When evaluating classifiers it is common to use accuracy, precision and recall as well as Hamming loss.  It has been shown that when calculating precision and recall on multi-label classifiers, it can be advantageous to use micro averaged precision and recall BIBREF6 . The formulas for micro averaged precision are expressed as DISPLAYFORM0 DISPLAYFORM1",what evaluation metrics are discussed?,What are the methods used to evaluate multi-label classifiers?,0.14915317296981812
task461-f55f5941542f4a379cc6268582c6da3d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:

MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.

CR: Sentiment prediction of customer product reviews BIBREF26.

SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.

MPQA: Phrase level opinion polarity classification from newswire BIBREF28.

SST: Stanford Sentiment Treebank with binary labels BIBREF29.

TREC: Fine grained question-type classification from TREC BIBREF30.

MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.",What transfer learning tasks are evaluated?,Which other sentence embeddings are used?,0.0884280800819397
task461-ff4b1c2294a74432825c7801f2d5e6aa,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In our experiments, we used WordNet 3.0 BIBREF9 as our external knowledge base INLINEFORM0 . For word embeddings, we experimented with two popular models: (1) GloVe embeddings trained by BIBREF10 on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), and (2) w2v-gn, Word2vec BIBREF5 trained on the Google News dataset (vocab: 3M, dim: 300). Our coverage enhancement starts by transforming the knowledge base INLINEFORM0 into a vector space representation that is comparable to that of the corpus-based space INLINEFORM1 . To this end, we use two techniques for learning low-dimensional feature spaces from knowledge graphs: DeepWalk and node2vec. DeepWalk uses a stream of short random walks in order to extract local information for a node from the graph. By treating these walks as short sentences and phrases in a special language, the approach learns latent representations for each node. Similarly, node2vec learns a mapping of nodes to continuous vectors that maximizes the likelihood of preserving network neighborhoods of nodes. Thanks to a flexible objective that is not tied to a particular sampling strategy, node2vec reports improvements over DeepWalk on multiple classification and link prediction datasets. For both these systems we used the default parameters and set the dimensionality of output representation to 100. Also, note than nodes in the semantic graph of WordNet represent synsets. Hence, a polysemous word would correspond to multiple nodes. In our experiments, we use the MaxSim assumption of BIBREF11 in order to map words to synsets.",What other embedding models are tested?,What datasets are used?,0.11387413740158081
task461-92ceee5092804a9bb20bbcb57feac0f5,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","They used 20-million-words data randomly sampled from the raw text released by the CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings BIBREF8, which is a combination of Wikipedia dump and common crawl.  For example, we compared the Latvian model by ELMoForManyLangs with a model we trained on a complete (wikidump + common crawl) Latvian corpus, which has about 280 million tokens.",How larger are the training sets of these versions of ELMo compared to the previous ones?,How big is the dataset used?,0.1393594741821289
task461-11a490c52da9452e9683c23422ff67eb,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","To do so, we constructed a new dataset with 23,700 queries that are short and unstructured, in the same style made by real users of task-oriented systems. ",What is the size of this dataset?,What datasets are used?,0.10821723937988281
task461-94242bb39e23449b8ad5c32bc1e56f1c,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In this work, we use MLP (modified to handle our data representation) as the base classifier.",Which classification algorithm do they use for s2sL?,What is the baseline used?,0.21866273880004883
task461-361e664f3a5d4d73a33820b305c66b3a,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey.",Which sports clubs are the targets?,What are the two football clubs that they used?,0.21503037214279175
task461-dbe837b7c04e401eb38906fa7953a0f8,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. Article-Section Ground-truth. The dataset consists of the triple INLINEFORM0 , where INLINEFORM1 , where we assume that INLINEFORM2 has already been determined as relevant. We therefore have a multi-class classification problem where we need to determine the section of INLINEFORM3 where INLINEFORM4 is cited. ",How do they determine the exact section to use the input article?,What is the dataset used?,0.08650487661361694
task461-96f234e2d35142aab578b07bb3899dbe,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","For sentiment classification, we systematically study the effect of character-level adversarial attacks on two architectures and four different input formats.  We also consider the task of paraphrase detection.",What end tasks do they evaluate on?,What adversarial attacks are used?,0.0697372555732727
task461-b71cc731e3af44998a3a693019ad6883,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295.",How big is the dataset?,What datasets are used?,0.38491594791412354
task461-eeeeb938de424ac7a28af926ca1926aa,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges).",Did they use a relation extraction method to construct the edges in the graph?,What are the types of edges that are used?,0.17029094696044922
task461-6e35539070d24a6ba2722b226143ce9d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Methods ::: Combining the two methods
We further propose to use the two methods together to combine their strengths. In fact, while the length token acts as a soft constraint to bias NMT to produce short or long translation with respect to the source, actually no length information is given to the network. On the other side, length encoding leverages information about the target length, but it is agnostic of the source length.",Do they experiment with combining both methods?,What are the two methods used?,0.09497040510177612
task461-6300b9b8baa348668d7a1c40935535b9,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The results in Table TABREF38 confirm the results of BIBREF13 and suggest that we successfully replicated a large proportion of their features. The results for all three prediction settings (one outgoing edge: INLINEFORM0 , support/attack: INLINEFORM1 and support/attack/neither: INLINEFORM2 ) across all type variables ( INLINEFORM3 , INLINEFORM4 and INLINEFORM5 ) are displayed in Table TABREF39 . All models significantly outperform the majority baseline with respect to macro F1.",What baseline and classification systems are used in experiments?,What are the baselines?,0.059389472007751465
task461-285d1c77ca3149c6b1d97a51e5debe03,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In the multiple-choice setting, which is the variety of question-answering (QA) that we focus on in this paper, there is also pragmatic reasoning involved in selecting optimal answer choices (e.g., while greenhouse effect might in some other context be a reasonable answer to the second question in Figure FIGREF1, global warming is a preferable candidate).",Do they focus on Reading Comprehension or multiple choice question answering?,What is the optimal answer choice?,0.11487686634063721
task461-549581afe7fc4b9bba5c68960f00594d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 . We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.",What models are used in the experiment?,How are the models trained?,0.1737208366394043
task461-bbf281661a9b48458af06ff01efff82e,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Chatino is a group of languages spoken in Oaxaca, Mexico. Together with the Zapotec language group, the Chatino languages form the Zapotecan branch of the Otomanguean language family. ",Which language family does Chatino belong to?,What is the language family of the language group?,0.10025769472122192
task461-63d2a28b4cdd476b86b1f567bc039a4d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",This section reports the results of the experiments conducted on two data sets for evaluating the performances of wDTW and wTED against other baseline methods. We denote the following distance/similarity measures. WMD: The Word Mover's Distance introduced in Section SECREF1 . VSM: The similarity measure introduced in Section UID12 . PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1 is the PV embedding of paragraph INLINEFORM2 . PV-TED: PV-TED is the same as Algorithm SECREF23 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 .,Which are the state-of-the-art models?,What are the baselines?,0.1085476279258728
task461-e02649ac707e4089a49b69455505cc22,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Defining Faithfulness ::: Assumption 1 (The Model Assumption).
Two models will make the same predictions if and only if they use the same reasoning process. Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).
On similar inputs, the model makes similar decisions if and only if its reasoning is similar. Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).
Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.",Which are three assumptions in current approaches for defining faithfulness?,What are the three assumptions?,0.16885441541671753
task461-414800fa899a46fa95bc2e54363a5ad9,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In this work, we explore the applicability of our method to three popular architectures of this layer: the LSTM-based, the CNN-based, and the transformer-based. Table TABREF46 shows performance of our method with different sequence modeling architectures. From the table, we can first see that the LSTM-based architecture performed better than the CNN- and transformer- based architectures. ",Which are the sequence model architectures this method can be transferred across?,What sequence models are used?,0.061911582946777344
task461-3d85cbd445bc40c7bf2eb70b347dd265,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus. ,What user variations have been tested?,How many crowdworkers were used?,0.06841546297073364
task461-265e6a5ef09542b687ddfc5235505dc4,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",DeepMine is publicly available for everybody with a variety of licenses for different users. It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. ,how was the speech collected?,How was the data collected?,0.18680983781814575
task461-93bd777f9b5f4c4e9df2fbdf5021a537,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","For each qualifying diagnosis tweet we retrieve the timeline of the corresponding Twitter user using the Twitter user_timeline API endpoint . Subsequently, we remove all non-English tweets (Twitter API machine-detected“lang” field), all retweets, and tweets that contain “diagnos*” or “depress*”, but not a valid diagnosis statement. The resulting Depressed cohort contains 1,207 individuals and 1,759,644 tweets ranging from from May 2008 to September 2018.",Do they report results only on English datasets?,How long did the dataset last?,0.16213726997375488
task461-6eebd8b1f0f643329300663787a2c0f0,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. ",What approach does this work propose for the new task?,What is the proposed model?,0.05634266138076782
task461-469e1b7c2b664508982ad91dbc0d063a,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We use the public financial news dataset released by BIBREF4, which is crawled from Reuters and Bloomberg over the period from October 2006 to November 2013. We conduct our experiments on predicting the Standard & Poor’s 500 stock (S&P 500) index and its selected individual stocks, obtaining indices and prices from Yahoo Finance. Detailed statistics of the training, development and test sets are shown in Table TABREF8. We report the final results on test set after using development set to tune some hyper-parameters.",What is dataset used for news-driven stock movement prediction?,What datasets are used?,0.27458035945892334
task461-262f24e457b24de8a7245b7ec9be0b7d,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In test batch 4, our system (called FACTOIDS) achieved highest recall score of ‘0.7033’ but low precision of 0.1119, leaving open the question of how could we have better balanced the two measures.",What was their highest recall score?,How is the performance of the model compared to the baseline?,0.11039143800735474
task461-ca54f91ba0fb4308868e770cf84c1e68,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.",A total of 27 different genres were scraped.,how many movie genres do they explore?,How many different genres were scrapped?,0.1593899130821228
task461-57058e94679c48f196554e34d87f1b08,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The multi-task model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%. The results differ between the tasks, but none have an overall benefit from the open vocabulary system.",Does the performance increase using their method?,How does the multi-task model perform?,0.08089137077331543
task461-f2a7f215b16543d886135f4957ef8ae3,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes.",How many demographic attributes they try to predict?,What demographic attributes are used?,0.058696091175079346
task461-3e577842b2734a66a60e28f5ab345988,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","We adopt several standard metrics widely used in existing works to measure the performance of dialogue generation models, including BLEU BIBREF24, embedding-based metrics (Average, Extrema, Greedy and Coherence) BIBREF25, BIBREF26, entropy-based metrics (Ent-{1,2}) BIBREF0 and distinct metrics (Dist-{1,2,3} and Intra-{1,2,3}) BIBREF1, BIBREF6.",What automatic evaluation metrics are used?,What metrics are used to measure the performance of the model?,0.12716925144195557
task461-d83412e85593452f84bcacf603c770a4,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","As a baseline for the RNN models, we apply a uni-directional RNN which predicts the relation after processing the whole sentence.",Which variant of the recurrent neural network do they use?,What baseline model do they use?,0.1437273621559143
task461-5fe0cb3437854660b2b534a11936487b,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","SVM: We define 3 sets of features to characterize each question. The first is a simple bag-of-words set of features over the question (SVM-BOW), the second is bag-of-words features of the question as well as length of the question in words (SVM-BOW + LEN), and lastly we extract bag-of-words features, length of the question in words as well as part-of-speech tags for the question (SVM-BOW + LEN + POS). This results in vectors of 200, 201 and 228 dimensions respectively, which are provided to an SVM with a linear kernel. No-Answer Baseline (NA) : Most of the questions we receive are difficult to answer in a legally-sound way on the basis of information present in the privacy policy. We establish a simple baseline to quantify the effect of identifying every question as unanswerable. Word Count Baseline : To quantify the effect of using simple lexical matching to answer the questions, we retrieve the top candidate policy sentences for each question using a word count baseline BIBREF53, which counts the number of question words that also appear in a sentence. We include the top 2, 3 and 5 candidates as baselines. Human Performance: We pick each reference answer provided by an annotator, and compute the F1 with respect to the remaining references, as described in section 4.2.1. Each reference answer is treated as the prediction, and the remaining n-1 answers are treated as the gold reference. The average of the maximum F1 across all reference answers is computed as the human baseline.",Were other baselines tested to compare with the neural baseline?,What baselines are used?,0.1826663613319397
task461-efcbcb4db5a64c73ab003d2a7fc0c3a1,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","Spoken-SQuAD is chosen as the target domain data for training and testing. Spoken-SQuAD BIBREF5 is an automatically generated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD BIBREF1 . There are 37,111 and 5,351 question answer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%.

The original SQuAD, Text-SQuAD, is chosen as the source domain data, where only question answering pairs appearing in Spoken-SQuAD are utilized. In our task setting, during training we train the proposed QA model on both Text-SQuAD and Spoken-SQuAD training sets. While in the testing stage, we evaluate the performance on Spoken-SQuAD testing set.",Which datasets did they use for evaluation?,What datasets are used?,0.09599423408508301
task461-55c199650fbb4e3d841e643085259ab2,task461_qasper_question_generation,Question Generation,"In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.","The number of fake news that have a duplicate in the training dataset are 1018 whereas, the number of articles with genuine content that have a duplicate article in the training set is 322.",what is the size of the dataset?,How many articles are duplicates in the training set?,0.18916916847229004
