{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3294a76-815e-4d06-8a97-554d720fd43a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T07:21:18.733335Z",
     "iopub.status.busy": "2025-11-03T07:21:18.733121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 07:21:29.861612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762154489.885778    1256 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762154489.895844    1256 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-03 07:21:30.101898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset lazily with Dask...\n",
      "Dataset has 98,417 rows across 500 partitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partitions:   0%|          | 2/500 [01:20<5:27:49, 39.50s/it]"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "PARQUET_FILE = \"natural_instructions_sample_balanced.parquet\"\n",
    "OUTPUT_FILE = \"llm_inference_results.parquet\"\n",
    "MODEL_NAME = \"google/flan-t5-large\"  # encoder-decoder model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "TARGET_BATCH_SIZE = 4   # adjust based on GPU memory\n",
    "MAX_NEW_TOKENS = 128\n",
    "N_PARTITIONS = 500       # number of Dask partitions\n",
    "\n",
    "CHUNKED_SAVE = True      # save after each partition to avoid large RAM usage\n",
    "SAVE_DIR = \"results_chunks_\"+MODEL_NAME.replace(\"/\", \"_\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# include targets now\n",
    "cols = [\"id\", \"task_name\", \"task_family\", \"definition\", \"inputs\", \"targets\"]\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load model & tokenizer\n",
    "# -----------------------\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\"  # automatically maps layers across CPU/GPU\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# 2. Read Parquet lazily with Dask\n",
    "# -----------------------\n",
    "print(\"Reading dataset lazily with Dask...\")\n",
    "ddf = dd.read_parquet(PARQUET_FILE, columns=cols).repartition(npartitions=N_PARTITIONS)\n",
    "print(f\"Dataset has {len(ddf):,} rows across {ddf.npartitions} partitions\")\n",
    "\n",
    "# -----------------------\n",
    "# 3. Batch inference per partition\n",
    "# -----------------------\n",
    "all_results = []\n",
    "\n",
    "for part_idx in tqdm(range(ddf.npartitions), desc=\"Processing partitions\"):\n",
    "    chunk_df = ddf.get_partition(part_idx).compute()\n",
    "    \n",
    "    if len(chunk_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Concatenate definition + inputs only for LLM input\n",
    "    llm_texts = (chunk_df[\"definition\"] + \" \" + chunk_df[\"inputs\"]).tolist()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(llm_texts):\n",
    "        batch_size = TARGET_BATCH_SIZE\n",
    "        batch_success = False\n",
    "        \n",
    "        while not batch_success:\n",
    "            try:\n",
    "                batch_texts = llm_texts[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize\n",
    "                inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "                \n",
    "                # Run model & measure latency\n",
    "                start_time = time.time()\n",
    "                outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # Decode\n",
    "                batch_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                batch_latency = (end_time - start_time) / len(batch_texts)\n",
    "                \n",
    "                # Save results keeping definition & inputs separate, plus targets\n",
    "                for j, text in enumerate(batch_texts):\n",
    "                    all_results.append({\n",
    "                        \"id\": chunk_df.iloc[i+j][\"id\"],\n",
    "                        \"task_name\": chunk_df.iloc[i+j][\"task_name\"],\n",
    "                        \"task_family\": chunk_df.iloc[i+j][\"task_family\"],\n",
    "                        \"definition\": chunk_df.iloc[i+j][\"definition\"],\n",
    "                        \"inputs\": chunk_df.iloc[i+j][\"inputs\"],\n",
    "                        \"targets\": chunk_df.iloc[i+j][\"targets\"],       # <-- added\n",
    "                        \"output_text\": batch_outputs[j],\n",
    "                        \"latency_sec\": batch_latency\n",
    "                    })\n",
    "                \n",
    "                batch_success = True\n",
    "                i += batch_size\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    batch_size = max(1, batch_size // 2)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(f\"OOM detected. Reducing batch size to {batch_size}\")\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    # Optionally save per partition to avoid large RAM usage\n",
    "    if CHUNKED_SAVE:\n",
    "        part_file = os.path.join(SAVE_DIR, f\"results_part_{part_idx}.parquet\")\n",
    "        pd.DataFrame(all_results).to_parquet(part_file, index=False)\n",
    "        all_results = []  # reset for next partition\n",
    "\n",
    "# -----------------------\n",
    "# 4. If CHUNKED_SAVE=False, save all results at once\n",
    "# -----------------------\n",
    "if not CHUNKED_SAVE:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(f\"Saved results to {OUTPUT_FILE}\")\n",
    "\n",
    "print(\"Inference completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07214110-045c-40d4-ae30-71bf6ec2de1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
